# -*- coding: utf-8 -*-
"""CNN_TRAINING Part 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10YJ1NWNDYxEhpxKN1xS9VoN24upy5al8
"""



"""Pytorch Libraries for CNN
1. torchvision etc etc

"""

import torch
from torch import nn
import torchvision
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
print(torch.__version__)
print(torchvision.__version__)

# 1. Getting a dataset and setup training data
train_data = datasets.FashionMNIST(
    root="data", # where to download and data is the name of the folder where it saves it
    train=True, # It loads the train split set from the dataset eg 80% of the datset goes into training
    download=True, # Downloads the data from the internet in the environment if false and data not present it will give an error
    transform=ToTensor(), # Converts PIL images to tensor format
    target_transform=None # Converting in One hot encoding format or not
)
test_data=datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
    target_transform=None
)

len(train_data),len(test_data)

image,label=train_data[0]
image,label

class_names=train_data.classes# Gives you the no of columns you have in the dataset
class_names

class_to_idx=train_data.class_to_idx # Puts each column/class name to a value in a dictionary
class_to_idx

#Visualizing the images
import matplotlib.pyplot as plt
image,label=train_data[0]
print(f"Image Shape{image.shape}")
plt.imshow(image.squeeze())
plt.imshow(image.squeeze(),cmap="gray")

torch.manual_seed(42)
fig=plt.figure(figsize=(9,9))
rows,columns=4,4
for i in range(1,rows*columns+1):
  random_idx=torch.randint(0,len(train_data),size=[1]).item()
  img,label=train_data[random_idx]
  fig.add_subplot(rows,columns,i)
  plt.imshow(img.squeeze(),cmap="gray")
  plt.title(class_names[label])
  plt.axis(False)

#Turn Dataset in dataloader
from torch.utils.data import DataLoader
Batch_Size=32
train_dataloader=DataLoader(dataset=train_data,batch_size=Batch_Size,shuffle=True)
test_dataloader=DataLoader(dataset=test_data,batch_size=Batch_Size,shuffle=False)
train_dataloader,test_dataloader

import torch
from torch import nn

# Simple Flatten MLP model
class FlattenModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()  # turns [B, 1, 28, 28] â†’ [B, 784]
        self.fc = nn.Sequential(
            nn.Linear(784, 128),  # first hidden layer
            nn.ReLU(),
            nn.Linear(128, 10)    # output layer (10 classes)
        )

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc(x)
        return x

# Create model
model = FlattenModel()

# Test it on one batch
X, y = next(iter(train_dataloader))  # get one batch
print("Before:", X.shape)  # [32, 1, 28, 28]
out = model(X)
print("After:", out.shape)  # [32, 10]

from torch import nn

class FashionMNISTModel(nn.Module):
    def __init__(self, input_shape, hidden_units, output_shape):
        super().__init__()
        self.layer_stack = nn.Sequential(
            nn.Flatten(),  # 28x28 -> 784
            nn.Linear(in_features=input_shape, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_shape),
            nn.ReLU()
        )

    def forward(self, x):
        return self.layer_stack(x)

